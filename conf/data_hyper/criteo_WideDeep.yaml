model:
  model: WideDeep
  batch_norm: False
  hidden_units: [1000, 1000, 1000, 1000, 1000]
  hidden_activations: relu
  embedding_dim: 16
  embedding_dropout: 0
  embedding_regularizer: 1.0e-05
  net_dropout: 0.2
  net_regularizer: 0


optim:
  lr: 0.001